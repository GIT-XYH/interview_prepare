1> 分区列表
    Spark是基于海量数据分布式计算的场景设计的；
    会把数据分给多个task并行计算，因此，也就需要把数据划分成多个任务片（partition）
2> 计算函数
    一个用于对每个partition数据进行逻辑运算的函数；
    每个partition的计算逻辑是相同的
3> 依赖 RDD 列表
    存储着当前RDD所依赖的一个或多个前序RDD
4> [可选] key-value类型RDD的分区器
5> [可选] 每个分区的首选执行位置