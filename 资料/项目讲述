// 自我介绍
各位面试官好, 我是许远航, 毕业于东华理工大学信息与计算科学专业, 我来面试公司的大数据开发工程师岗位, ,上一家公司为上海识装信息科技有限公司, 在其数据部门担任大数据开发工程师, 主要负责技术调研, 需求沟通, 代码开发以及相关的平台调优等.


// 项目一整体介绍
首先我先讲一下我最近半年多做的一个项目, 实时规则营销推送系统, 属于一个事件驱动型系统. 这个系统相比通常的数据统计类项目来说, 更具备业务价值: 他是直接服务于我们的市场运营的, 运营的小伙伴对这个项目的评价也是相当不错的.
简单来说, 我们这个系统, 可以在检测到用户群体做出的某些行为时, 根据营销规则, 及时向用户推送短信或 app 弹窗消息, 有点类似于神测数据的智能运营这个产品.技术方面, 用到的框架不是很多, 主要有 flink, drools, redis, clickhouse 等等; 但是系统内部的落地实现和很多细节方面的设计点还是挺多的;目前, 系统内同时在线运行的运营规则大约有六十多个, 规则的平均响应时间在 5ms 左右;
对项目的整体架构来说, 埋点日志采集系统和规则管理系统主要由前端和运维相关小组完成的.我们需要做的是将时间明细数据和规则表操作 binlog 从 kafka 拉取过来, 将事件进行动态 keyBy 分组, binlog 放入规则注入模块对规则进行解析和控制, 然后将动态分组模块和规则注入模块处理之后的数据传递到规则计算引擎中, 在规则计算引擎经过 Controller 层, Service 层和 Dao 层的处理之后, 和我们的数据存储层和缓存管理系统匹配对比, 最后规则计算结果输出

这是项目的一个整体情况, 项目的具体内容较多, 主要分为这么几块, 我来为您详细说明一下.

我们重点使用的存储系统有如下几种:

1> ClickHouse 存储行为事件明细数据, 全量存储
2> Hbase 用户画像标签存储
3> FlinkState 近期行为事件明细数据存储
4> Redis 缓存数据存储
5> MySql 规则数据及系统元数据存储

数据处理的整体流程我简历上有详细说明, 这里就不在一一概述.

规则的核心计算流程, 在系统内分为 controller 层, service 层和 dao 层, 下面我来为您详细说明一下这三层
1>controller 层
这一层主要负责规则中的多个条件的逻辑与或非以及计算先后顺序等组织.
比如，一个规则有：条件①、条件②、条件③、条件④
而且该规则对这些条件的逻辑要求是： 
（（条件①  || 条件② ）&& 条件③ ） || 条件④
本层为了使用这些变化无穷的条件组合逻辑, 使用了 DROOLS 规则引擎, 将这一层的逻辑做成了随规则动态注入.具体来说, 就是将这些条件组合逻辑, 用 DROOLS 的规则文件 Drl 进行表达;因而可以随规则条件参数动态注入到系统, 以及动态调用.
这里最开始的时候我们选用的 Flink 比较火地 CEP 实时复杂事件处理, 我们经过调研实验, 发现 CEP 不够灵活, 规则一旦确定, 如果想要修改, 就必须停止原来的job, 然后重新编码修改规则

2> service 层
这一层主要提供各类规则条件的计算方法, 比如:
如画像条件匹配计算方法
单个行为发生次数条件的匹配计算方法
行为组合发生次数类条件的匹配计算方法等
在这些方法中, 会自动根据查询条件的时间范围来控制, 控制其是否需要分段查询, 是否需要在缓存中查询, 查询过程中对查询条件进行调整等等...
对于分段查询来说, 我们在项目的过程中有一些比较精巧的设计, 您看我需要详细的给您讲述一下吗?

/
分段查询的基本思想是一个规则, 通常都会带上时间范围, 比如: 在最近五分钟内发生行为 A 超过三次, 在某个具体的时间范围内发生 A 超过 3 次, 这些是啊金范围跨度又长又短, 有固定范围(如几月几号到几月几号), 也有不断推移的范围(如最近五分钟)
那么, 在具体计算时, 我们该如何选择去哪里查询呢?
如果仅仅在 state 中查询, 不能完全满足, 因为 state 中只存在了最近 3 小时的数据
如果仅仅在 clickhouse 中查询, 也不能完全满足, 抛开对数据库压力不说, 在 clickhouse 中的数据比实际的数据可能少了最近几十秒甚至少了分钟级别的数据, 这对我们实时项目来说非常不好.
我们的解决方案是, 将条件的时间范围, 划分为近期和远期进行分段查询, 具体来说, 就是设计一个时间分界点, 然后看条件的时间跨度和这个分界点之间的对比关系, 来决定如何查询.
	完整落在分界点左边的： 只需要查询clickhouse
	完整落在分界点右边的： 只需要查询state
	跨分界点的：就划分成两个区间进行组合查询
上述分段查询中, 分界点的设计极为重要:
如果只是把分界点定义为当前时间倒退一小时, 则这个分界点会持续漂移, 不利于后续的缓存设计, 因此, 我们要设法降低分界点的漂移频率
采用的方案为, 将分界点定义为当前时间向上取整倒退一小时, 这样, 分界点的漂移将降低成一小时才漂移一次
上述就是我们初步的分段查询设计思想
/
3> dao 层
这一层主要提供对底层数据存储的各类原子条件查询功能, 比如:
	查询画像条件是否匹配
	查询单个事件的发生次数
	事件组合的发生次数
	事件组合的过滤字符串序列等
在本层中, 为了让系统对事件组合类条件的查询设计更加灵活和通用, 我们将单个事件以及多事件组合条件的查询做成了正则化查询, 比如
	条件举例1： 发生过行为A后，隔两个任意其他行为，发生行为B，任意事件后，发生行为C
	条件举例2： 发生过行为A后，隔任意事件后，连续发生3次行为B，任意时间后，发生行为C
	条件举例3： 发生过行为A后，隔2个任意事件后，发生行为B，任意事件（不能有X）后，发生行为C
上述条件灵活多变, 如果用 java逻辑去实现, 则很难做到通用, 于是我们设计了正则化查询的方案
像条件 1 和条件 2 关心的事件集为{ABC}, 条件 3 关心的事件集为{ABCD},
而各个事件之间的间隔关系, 连续规律等, 完全可以使用正则表达式来进行表达和匹配
	举例条件1的正则表达式：   A[A-Z]{2}B.*C
	举例条件2的正则表达式：   A.*B{3}.*C
	举例条件3的正则表达式：   A[A-Z]{2}B(?!X).*C
从而, 要进行上述条件的计算判断时, 可以做成如下流程
	1. 从数据存储中, 过滤出所关心的数据集, 而且只抽取事件的 eventId, 形成一个满足条件的时间 id 字符串序列
	2. 然后, 拿着条件所对应的正则表达式, 去匹配上面的字符串序列即可
这样一来, 不仅可以提高规则定义的灵活可扩展性, 还顺带降低了对 clickhouse 的查询压力, 因为我们不在需要直接在 clickhouse 中计事件序列是否满足规则定义, 而是只要从 clickhouse 中过滤查询出规则条件所关心的事件集 jiu 行了
但是, 在调试运行中, 我们发现上述存在有时候可能会存在一些问题,比如一个事件 A, 不光是一个单独的事件,而是一个嵌套的事件id, 这样会导致正则表达式的编写异常困难, 我们实验讨论后得出的解决方案是对事件 id 进行编码, 将事件 id 字符串变成"12312"这样的码串.从而对应的正标表达式就可以非常简洁



规则的三层核心计算流程我已经初步概述完成了, 接下来我和您讲述一下规则注入模块
规则注入的核心机理有以下五点
	1> 规则管理平台 web 平台操作规则(如新建, 删除, 启用, 停用等等), 导致 mysql 中规则元数据表的变化
	2> cananl 监听到规则元数据表的操作 binlog 并发送到 kafka
	3> flink 从 kafka 消费到规则操作 binlog, 并将 binlog 流进行广播后 connect 事件数据流
	4> 在后续的处理中, 通过 processBroadcast 方法, 读取到规则操作 binlog 并进行解析
	5> 根据解析的结果, 对存储规则信息用 broadcastState 进行管理
规则注入的时机是在动态 keyBy 之前, 因为动态 keyBy 的时候, 需要获取到当前系统中所运行的所有规则
还有就是规则在 flink 内存中的存储模型, 此处的核心设计思想是把规则相关的信息(如规则名称, 规则参数对象, 规则 controller 对应的 drl 逻辑所构成的 kiesession), 统一成一个 pojo 后存放在广播的 broadcastState 中




接下来我要说的是动态分组模块
首先是动态 keyBy 的需求
有些规则的计算需要将数据按照 deviceId 分组, 如: "满足某些条件的用户出发了某行为, 则..."
有些规则的计算则需要将数据按照 ip 分组, 如: "一个 ip 在 5 分钟内连续发生注册事件超过 10 次, 则..."
因此, 系统需要根据规则来进行动态 keyBy
动态 keyBy 的核心数据模型是一个标记了动态 key 的 eventBean
动态 keyBy 的核心机理
一个 event 数据, 要按照不同的分组要求进行分组, 那么, 同一个 event 数据必然需要同时进入多个分组, 从而, 动态,keyby 的核心机理, 就是将同一个 event 数据, 根据规则的分组种类, 复制成多个 event 数据并标记上不同的分组 key 值, 并形成一个 DynamicKeyBean



接下来要讲述的是查询缓存系统
首先, 我来说一下引入缓存的原因
	1> 同一个规则, 可能被同一个用户多次触发计算, 从而规则中的各个条件都要被重复计算(反复查询 clickhouse 等)
	2> 不同的规则, 可能有相似的条件, 从而也会引起这些相似的条件的重复计算
如果我们能避免这些重复计算, 就可以极大降低 clickhouse 的查询压力, 从而极大提升系统整体的响应能力
基本思想是将各个规则中各条件的查询结果, 缓存在内存数据库 redis 中, 当然, 也可以选择 flink 的 rocksdb 状态, 当这些条件需要进行再计算时, 可以直接从缓存中获取计算结果, 而不需要每次都去查询 clickhouse
这里边的关键设计师缓存数据模型, 他密切影响着缓存的可用性, 一个最直观的想法是:
	将规则的动态 key+条件设置为缓存 key, 将条件查询的结果设置为 value.但是我们这样设计有一个重大问题, 就是里边的条件很难表达出来, 因为条件本身就是一个非常复杂的信息题, 包括事件 id, 事件属性以及事件组合规律等等, 如果把这个复杂的信息体放入缓存 key, 这个 key 就会过于复杂和冗长, 因此, 我们有了一个精妙的设计:
	对所有规则中的所有事件组合条件, 设置一个全局唯一的整数 id(这个可以在规则平台生成规则时, 分配这个唯一 id)
而且, 由于我们在 dao 层对事件组合条件做了正则化查询设计,所以, 就算两个不同的事件组合条件, 只要他们所关心的事件集相同, 则他们的缓存条件 id 就可以相同, 因为这些条件的查询结果都只是所关心的事件集过滤出来的实际事件序列, 比如:
	条件1：  所关心的事件集{1,2,3,4}，事件组合规律正则：{.*1.*2.*3}
	条件2：  所关心的事件集{1,2,3,4}，事件组合规律正则：{.*1{2).*23}
	虽然他们的正则模型不同, 但是底层查询的返回事件序列是完全可以共用的, 所以, 这两个条件的缓存 id 应该完全一致的
这样一来, 我们不仅可以实现相同条件查询结果的缓存共用, 更进一步实现了相似条件查询结果的缓存共用, 因而, 我们得出的数据模型为
key是 deviceId+条件 id, value 是时间范围+查询结果+缓存插入时间. (加入缓存插入时间, 是为了方便缓存 TTL 管理)

这里呢, 有一个问题, 如果每次条件查询, 都要去 redis 中请求缓存数据的话, 对 redis 的压力也是非常大的, 不过我们的缓存设计模型做的比较到位, 可以再缓存查询之前, 方便的增加一个 bitmap 来降低缓存的查询频率, bitmap 的维护机制如下:
	1> 在KeyedProcessFunction中，维护一个bitmap放在state中；
	2> 条件查询得到结果存入缓存后，同时在bitmap中进行记录该条件的条件id（这里很方便，因为条件id本身是一个整数，可以轻松记入bitmap）；
	3> 如果缓存中某条件id的缓存数据被清空，则从bitmap中清除这个条件id；
这样一来, 下次请求缓存钱, 可以先检查一下 bitmap 中是否有当前条件的 id 存在
举个例子, 查询缓存和 ck 的情况

(运行监控系统
metric 指标收集指标可视化我没有做具体参与
)
上述就是我对近期项目的大体描述, 请问我现在要讲述第二个项目吗, 第二个比较简单, 主要是辅助我们的直播平台做一些指标统计分析




项目一: 实时营销项目 项目周期 6-8 个月
项目二: 3-5 个月
离线数仓: 1-2 年


技术调研:
cep: 复杂事件处理, 规则注入后就会写死
技术测试:



第一个做的需求是对新增用户数量做数据统计, 最开始我们计划的是指用 flink 进行处理, 实现思路是直接在 flink 中先进行 keyBy, 然后 sum, 再 sink, 正常单维度下没有问题, 但是当增加例如省份地区等多维度统计之后, 这样做代码就会太冗余, 要调用很多次 keyBy, sum 以及 sink, 程序的执行速度变慢, 占用更多资源, 吞吐量低

-- 多维度的复杂统计

我们发现发,flink 多维度分析这个短板, 讨论分析想到 clickhouse 很适合处理多维度数据, 我们将这些数据存储在 clickhouse 中, 但是 clickhouse 不支持事务, 做不到 exactlyOnce, 但是呢, 我们通过一些手段让他实现了 atLeastOnce.

什么时候会有重复的数据产生?
在两个检查点之间, 机器宕机了, 数据由于要写入到 clickhouse 中, 他就会重新从检查点进行读取数据, 会将数据再次写入到 clickhouse 中, 所以要保证只保留最新的数据(也就是 atLeastOnce), 否则就会有重复数据, 对于这个问题, 我们讨论研究, 决定使用 ReplaceingMergeTree 引擎

思路:
1. 使用 clickhouse 的 ReplacingMergeTree, 可以将同一个分区中, 主键相同的数据进行 Merge, 可以保留最新的数据, 这样从检查点读数据时, 最新的数据就会覆盖原来的数据
2. 那么如何设计表就成了关键, 主键是什么, 按照什么分区, 按照什么字段来合并分区
3. 先来说主键的设计, 由于 ReplacingMergeTree 以后会合并分区内主键相同的数据, 为了保证每一条数据都要进入到 clickhouse中, 所以我们必须保证每一条数据都要有唯一的标识来保证每条数据进入分区的时候不会被合并,然而我们处理的数据中没有这样的字段, 我们经过各种实验讨论, 选择从 kafka 读数据的时候把数据的 topic+partition+偏移量当做这条数据的唯一标识,
4. 再来说分区字段, 由于我们要保证相同时间范围的数据再同一个分区, 所以建表的时候, 按照数据携带的时间进行分区, 如日期, 小时等
5. 对于合并分区的时候, 我们选择系统中当前时间来合并分区, 这样即使机器宕机, 从检查点读到重复数据, clickhouse 会保留最新的数据, 这样就实现了 atleastonce.
6. 然而还有一些问题: 我们将数据写入到 clickhouse 之后, 数据不能立即,merge, 需要手动 optimize 或者后台自动合并.
7. 我们对于数据精度要求比较高的数据, 查询时在表明后边加上了 final 关键字, 就能查询到最新的数据, 只不过效率会稍微变低.但是我们要求的不是很发哦, 一般不用添加, 直接查询, 因为机器挂掉的几率也不是特别大
8. 使用以上的特性, 我们勉强实现了数据的 AtLeastIOnce, 保证数据的一致性. 


还有一些对于直播间新老用户的统计判断, 我们讨论出来了三种方法
1. 按照设备 ID 进行 keyBy, 使用 keyProcessFunction, 使用状态, 装的是布隆过滤器, 每个设备 id 对应一个布隆过滤器, 这样有一个问题, 就是随着用户数据量越来越多, 定义的布隆过滤器也越来越多, 内存最后会溢出
2. 按照设备 类型进行 keyBy, 这个时候一种设备类型对应一个布隆过滤器, 这样虽然不会内存溢出, 但是同一个分区数据可能太多, 造成数据倾斜
3. 按照设备 ID 进行 keyBy, 不同的是, keyBy 之后调用 map 方法, 使用 operateState, 每个分区一个 listState 状态, 状态中装的只有一个布隆过滤器, 这样及解决了数据倾斜, 也不会有内存溢出

那是如何判断的?
布隆过滤器的特点是: 如果 key 不在布隆过滤器中, 那他一定是不存在的, 那么 每个设备 id 的数据来第一条的时候都不在布隆过滤器中, 所以把这条数据标记为新用户, 其他情况都是 0, 这样就得到了新老用户的标记
通过这样做, 我们不但解决了数据倾斜问题, 也解决了占用太多内存的问题, 因为只是用了一个布隆过滤器来存储设备是否存在, 并且布隆过滤器占用的空间还很小


需求二: 实时统计每个播主直播的观看人数(要求直播间至少停留一分钟, 在三十分钟内, 同一个设备 id 频繁进入该直播间, 算一个用户的人气值)

讨论得出两种实现方式:
实现方式一比较极端, 为了达到低延迟的目的, 将数据每来一条就写入数据库中
思路:
 1. 筛选出 liveEnter 和 liveLeave 的事件数据
 2. 按照主播 id 进行分组
 3. 组内使用 mapState 保存 liveEnter 的数量和 liveLeave 的数量, 1. 事件是liveEnter，对应的value+1，事件是liveLeave，它对应的value值-1。初始状态全部是0.
 4. 将结果来一条就输出到Redis

 实现方式二: (优化)
 为了减少对数据库的压力, 我们使用窗口, 将数据进行增量聚合, 这样输出的数据变少了, 对数据库的压力也就变小了, 先用一个聚合, 再用一个 windowFunction, 当窗口触发的时候在将数据写入到 Redis 中.

 需求三: 直播礼物的实时统计分析
 统计的具体指标
1.各个主播收到礼物的数量、积分值
3.做多维度的指标统计（ClickHouse）

思路:
需要统计的维度数据在 Mysql 中, 行为数据在 kafka 中
1. 先过滤出 liveReward(打赏记录)
2. 将较小的表广播出去
3. 大流 connect 小流, 两个流可以共享状态.
4. 根据直播间 id 进行 keyBy
5. reduce 统计出礼物数量和直播总积分
由于要查询小表的数据, 所以要把小表广播出去, 将两个流连接起来, 再做数据分析
用到的技术点:
- 广播 state
- reduce 的使用
- 从 mysql 等数据流中读取数据, 写入数据
- 从 kafkaStream 中读取数据等

需求四: 热门商品 Top 实时统计
需求: 统计十分钟内, 每隔一分钟统计一次各个分类, 各种事件类型的热门商品(商品 id)
我想统计最近十分钟之内, 每分钟统计一次, 那个手机分类, 哪一种事件类型浏览数数量最多的具体手机型号的 Top3
窗口选择: 统计十分钟, 每隔一分钟统计一次结果, 为了得到结果比较准确(平滑) 我们使用滑动窗口.窗口的长度和滑动步长可以根据实际情况进行调整(如果使用滚动窗口, 窗口的变化幅度范围太大, 统计结果不平滑)
事件类型: 为了精确的统计出用户在实际浏览, 加入购物车, 下单时的热门商品的信息的变化情况, 使用 eventTime 类型的窗口
实现步骤:
1. 现将数据按照分类 id, 时间 id, 商品 id 进行 keyBy.
2. 划分时间时间滑动窗口
3. 调用 aggregate 方法, 传入聚合方法, 在窗口内进行聚合, 累加计算出次数.(不适用全局聚合, 因为效率低, 占用大量资源).获得到(分类 id, 事件 id , 商品 id 和次数), 再传入一个 WindowFunction, 获取窗口的信息(窗口的起始时间和结束时间)这样窗口触发后就可以再 WindowFunction 获取到窗口聚合后的数据(次数),  并且也得到了窗口的起始时间, 结束时间
4. 接下来按照(分类 id, 事件 id, 窗口的起始时间, 结束时间)进行 keyBy.
5. 之后排序: 使用 ProcessFunction 的 onTimer 方法定时器进行排序, 每来一条数据, 不直接进行输出, 而是将数据存储到 State 中(为了容错), 再注册一个比当前窗口的结束时间大一毫秒的定时器, 当下一个窗口的数据触发时, 窗口的 WaterMark 已经达到了我们注册的定时器的时间, 于是触发 Timmer, 将上一个窗口的数据进行排序.
为什么设置窗口的结束时间+1ms
同一个窗口的数据来的时候, 他的水位线时间永远达不到窗口的结束时间+1, 所以一个窗口的数据还没有被排序看, 但是当下一个窗口的第一条数据到来的时候, 水位线的是啊金达到了窗口的结束时间+1, 这个时候定时器启动, 对当前存储在状态中的数据(第一个窗口的数据进行排序)如果不设置+1, name 第一个窗口的最后一条数据就达到了定时器的时间, 就开始排序, 最后一条数据会被丢失
用到的技术
- keyby
- 开窗(滑动窗口), 作用是统计数据相对平滑
- 窗口内聚合函数
- 窗口触发函数
- keyBy 之后的定时器 